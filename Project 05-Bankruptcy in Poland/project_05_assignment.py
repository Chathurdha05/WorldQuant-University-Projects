# -*- coding: utf-8 -*-
"""Project 05-Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dotQoeFRngdnGG-_tEkPWIahHSrmmUM2

# **5.5. Bankruptcy in Taiwan 🇹🇼**
"""

import wqet_grader

wqet_grader.init("Project 5 Assessment")

# Import libraries here
import gzip
import json
import pickle

import ipywidgets as widgets
import pandas as pd
import wqet_grader
from imblearn.over_sampling import RandomOverSampler
from IPython.display import VimeoVideo
from ipywidgets import interact
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    ConfusionMatrixDisplay,
    classification_report,
    confusion_matrix,
)
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import make_pipeline
from teaching_tools.widgets import ConfusionMatrixWidget

"""## **Prepare Data**

### **Import**

**Task 5.5.1:** Load the contents of the "data/taiwan-bankruptcy-data.json.gz" and assign it to the variable taiwan_data.

Note that taiwan_data should be a dictionary. You'll create a DataFrame in a later task.
"""

# Load data file
with gzip.open('data/taiwan-bankruptcy-data.json.gz', 'rt', encoding='utf-8') as f:
    taiwan_data = json.load(f)

# Check the type
print(type(taiwan_data))

wqet_grader.grade("Project 5 Assessment", "Task 5.5.1", taiwan_data["metadata"])

"""**Task 5.5.2:** Extract the key names from taiwan_data and assign them to the variable taiwan_data_keys.

**Tip:** The data in this assignment might be organized differently than the data from the project, so be sure to inspect it first.
"""

taiwan_data_keys = list(taiwan_data.keys())
print(taiwan_data_keys)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.2", list(taiwan_data_keys))

"""**Task 5.5.3:** Calculate how many companies are in taiwan_data and assign the result to n_companies."""

n_companies = len(taiwan_data["observations"])
print(n_companies)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.3", [n_companies])

"""**Task 5.5.4:** Calculate the number of features associated with each company and assign the result to n_features."""

n_features = len(taiwan_data['observations'][0])
print(n_features)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.4", [n_features])

"""**Task 5.5.5:** Create a wrangle function that takes as input the path of a compressed JSON file and returns the file's contents as a DataFrame. Be sure that the index of the DataFrame contains the ID of the companies. When your function is complete, use it to load the data into the DataFrame df."""

# Create wrangle function
def wrangle(filepath):
    # Open and load the JSON data
    with gzip.open(filepath, 'rt', encoding='utf-8') as f:
        data = json.load(f)

    # Convert observations to DataFrame
    df = pd.DataFrame(data['observations'])

    # Set ID column as index (assuming it's called 'company_id' or similar — inspect to confirm)
    if 'company_id' in df.columns:
        df.set_index('company_id', inplace=True)
    elif 'id' in df.columns:
        df.set_index('id', inplace=True)

    return df

df = wrangle('data/taiwan-bankruptcy-data.json.gz')
print("df shape:", df.shape)
df.head()

wqet_grader.grade("Project 5 Assessment", "Task 5.5.5", df)

"""### **Explore**

**Task 5.5.6:** Is there any missing data in the dataset? Create a Series where the index contains the name of the columns in df and the values are the number of NaNs in each column. Assign the result to nans_by_col. Neither the Series itself nor its index require a name.
"""

nans_by_col = df.isna().sum()
print("nans_by_col shape:", nans_by_col.shape)
nans_by_col.head()

wqet_grader.grade("Project 5 Assessment", "Task 5.5.6", nans_by_col)

"""**Task 5.5.7:** Is the data imbalanced? Create a bar chart that shows the normalized value counts for the column df["bankrupt"]. Be sure to label your x-axis "Bankrupt", your y-axis "Frequency", and use the title "Class Balance"."""

# Plot class balance
import matplotlib.pyplot as plt
df["bankrupt"].value_counts(normalize=True).plot(
    kind="bar",
    xlabel="Bankrupt",
    ylabel="Frequency",
    title="Class Balance"
);
# Don't delete the code below 👇
plt.savefig("images/5-5-7.png", dpi=150)

with open("images/5-5-7.png", "rb") as file:
    wqet_grader.grade("Project 5 Assessment", "Task 5.5.7", file)

"""### **Split**

**Task 5.5.8:** Create your feature matrix X and target vector y. Your target is "bankrupt".
"""

target = "bankrupt"
X = df.drop(columns=target)
y = df[target]
print("X shape:", X.shape)
print("y shape:", y.shape)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.8a", X)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.8b", y)

"""**Task 5.5.9:** Divide your dataset into training and test sets using a randomized split. Your test set should be 20% of your data. Be sure to set random_state to 42."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.9", list(X_train.shape))

"""### **Resample**

**Task 5.5.10:** Create a new feature matrix X_train_over and target vector y_train_over by performing random over-sampling on the training data. Be sure to set the random_state to 42.
"""

over_sampler = RandomOverSampler(random_state=42)
X_train_over, y_train_over = over_sampler.fit_resample(X_train, y_train)
print("X_train_over shape:", X_train_over.shape)
X_train_over.head()

wqet_grader.grade("Project 5 Assessment", "Task 5.5.10", list(X_train_over.shape))

"""## **Build Model**

### **Iterate**

**Task 5.5.11:** Create a classifier clf that can be trained on (X_train_over, y_train_over). You can use any of the new, ensemble predictors you've learned about in this project.
"""

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier(random_state=42)

clf.fit(X_train_over, y_train_over)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.11", clf)

"""**Task 5.5.12:** Perform cross-validation with your classifier using the over-sampled training data, and assign your results to cv_scores. Be sure to set the cv argument to 5.

**Tip:** Use your CV scores to evaluate different classifiers. Choose the one that gives you the best scores.
"""

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(clf, X_train_over, y_train_over, cv=5, n_jobs=-1)
print(cv_scores)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.12", list(cv_scores))

"""**Ungraded Task:** Create a dictionary params with the range of hyperparameters that you want to evaluate for your classifier. If you're not sure which hyperparameters to tune, check the scikit-learn documentation for your predictor for ideas.

**Tip:** If the classifier you built is a predictor only (not a pipeline with multiple steps), you don't need to include the step name in the keys of your params dictionary. For example, if your classifier was only a random forest (not a pipeline containing a random forest), your would access the number of estimators using "n_estimators", not "randomforestclassifier__n_estimators".
"""

params = {
    "n_estimators": range(25, 100, 25),
    "max_depth": range(10, 50, 10)
}

"""**Task 5.5.13:** Create a GridSearchCV named model that includes your classifier and hyperparameter grid. Be sure to set cv to 5, n_jobs to -1, and verbose to 1."""

model = GridSearchCV(
    clf,
    param_grid=params,
    cv=5,
    n_jobs=-1,
    verbose=1
)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.13", model)

"""**Ungraded Task:** Fit your model to the over-sampled training data."""

model.fit(X_train_over, y_train_over)

"""**Task 5.5.14:** Extract the cross-validation results from your model, and load them into a DataFrame named cv_results. Looking at the results, which set of hyperparameters led to the best performance?"""

cv_results = pd.DataFrame(model.cv_results_)
cv_results.head(5)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.14", cv_results)

"""**Task 5.5.15:** Extract the best hyperparameters from your model and assign them to best_params."""

best_params = model.best_params_
print(best_params)

wqet_grader.grade(
    "Project 5 Assessment", "Task 5.5.15", [isinstance(best_params, dict)]
)

"""### **Evaluate**

**Ungraded Task:** Test the quality of your model by calculating accuracy scores for the training and test data.
"""

acc_train = model.score(X_train_over, y_train_over)
acc_test = model.score(X_test, y_test)


print("Model Training Accuracy:", round(acc_train, 4))
print("Model Test Accuracy:", round(acc_test, 4))

"""**Task 5.5.16:** Plot a confusion matrix that shows how your model performed on your test set."""

ConfusionMatrixDisplay.from_estimator(model, X_test, y_test);
# Don't delete the code below 👇
plt.savefig("images/5-5-16.png", dpi=150)

with open("images/5-5-16.png", "rb") as file:
    wqet_grader.grade("Project 5 Assessment", "Task 5.5.16", file)

"""**Task 5.5.17:** Generate a classification report for your model's performance on the test data and assign it to class_report."""

from sklearn.metrics import classification_report
class_report = classification_report(y_test, model.predict(X_test))
print(class_report)

wqet_grader.grade("Project 5 Assessment", "Task 5.5.17", class_report)

"""## **Communicate**

**Task 5.5.18:** Create a horizontal bar chart with the 10 most important features for your model. Be sure to label the x-axis "Gini Importance", the y-axis "Feature", and use the title "Feature Importance".
"""

features = X_train_over.columns
importances = model.best_estimator_.feature_importances_
feat_imp = pd.Series(importances, index=features).sort_values()
feat_imp.tail(10).plot(kind="barh")
plt.xlabel("Gini Importance")
plt.ylabel("Feature")
plt.title("Feature Importance");

# Don't delete the code below 👇
plt.savefig("images/5-5-17.png", dpi=150)

with open("images/5-5-17.png", "rb") as file:
    wqet_grader.grade("Project 5 Assessment", "Task 5.5.18", file)

"""**Task 5.5.19:** Save your best-performing model to a a file named "model-5-5.pkl"."""

# Save model
with open("model-5-5.pkl", "wb") as f:
    pickle.dump(model, f)

with open("model-5-5.pkl", "rb") as f:
    wqet_grader.grade("Project 5 Assessment", "Task 5.5.19", pickle.load(f))

"""**Task 5.5.20:** Open the file my_predictor_assignment.py. Add your wrangle function, and then create a make_predictions function that takes two arguments: data_filepath and model_filepath. Use the cell below to test your module. When you're satisfied with the result, submit it to the grader."""

# Import your module
from my_predictor_assignment import make_predictions

# Generate predictions
y_test_pred = make_predictions(
    data_filepath="data/taiwan-bankruptcy-data-test-features.json.gz",
    model_filepath="model-5-5.pkl",
)

print("predictions shape:", y_test_pred.shape)
y_test_pred.head()

"""**Tip:** If you get an ImportError when you try to import make_predictions from my_predictor_assignment, try restarting your kernel. Go to the Kernel menu and click on Restart Kernel and Clear All Outputs. Then rerun just the cell above. ☝️"""

wqet_grader.grade(
    "Project 5 Assessment",
    "Task 5.5.20",
    make_predictions(
        data_filepath="data/taiwan-bankruptcy-data-test-features.json.gz",
        model_filepath="model-5-5.pkl",
    ),
)

"""Copyright 2023 WorldQuant University. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited."""